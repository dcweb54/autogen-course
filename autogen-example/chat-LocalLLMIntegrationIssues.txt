### USER
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent

# from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_core.models import UserMessage

import asyncio


# C:\Users\admin\AppData\Local\llama.cpp\ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf
async def main():
    # llama_client = LlamaCppChatCompletionClient(model_path="C:/Users/admin/AppData/Local/llama.cpp/ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf")
    # http://192.168.0.1:2600/v1
    open_client = OpenAIChatCompletionClient(
        model=" gemma-3-1b-it-GGUF",
        base_url="http://localhost:8080/v1",
        api_key="placeholder",
        model_info={
            "vision": False,
            "function_calling": False,
            "json_output": True,
            "family": "",
            "structured_output": True,
        },
        response_format={
            "type": "json_object",
            "schema": {
                "type": "object",
                "properties": {"capital": {"type": "string"}},
                "required": ["capital"],
            },
        },
    )
    result = await open_client.create(
        [UserMessage(content="how to drive a car", source="user")]
    )
    print(result.content)


if __name__ == "__main__":
    asyncio.run(main())


### ASSISTANT
The code you've written attempts to use `autogen_agentchat` with a local LLM server (likely **llama.cpp** or **LocalAI**) via the OpenAI-compatible API, but there are several **critical issues** that will prevent it from working as intended ‚Äî especially with the **response format** and **model capabilities**.

---

## ‚ùó Key Problems in Your Code

### 1. **Invalid Model Name**
```python
model=" gemma-3-1b-it-GGUF",
```
‚Üí There‚Äôs a **leading space**, and more importantly, **this is not a valid OpenAI model name**.

Even though you're pointing to a local server (`localhost:8080/v1`), the `OpenAIChatCompletionClient` may still validate or expect standard model names unless the backend ignores it. Some local servers (like **LocalAI**) require the actual model filename or alias configured in their settings.

‚úÖ **Fix**: Remove the space and ensure the model name matches what your local server expects. For example:
```python
model="gemma-3-1b-it",  # if that's how it's registered in your local server
```

---

### 2. **You‚Äôre Requesting JSON Output for an Unstructured Query**
You set:
```python
response_format={
    "type": "json_object",
    "schema": {
        "type": "object",
        "properties": {"capital": {"type": "string"}},
        "required": ["capital"],
    },
}
```

But you‚Äôre asking:
```python
"how to drive a car"
```

‚Üí This prompt has **nothing to do with capitals**. The model will be forced to output JSON with a `"capital"` field ‚Äî which is **nonsensical** for this query.

‚úÖ **Fix**: Either:
- Ask a question that fits the schema:  
  ```python
  "What is the capital of France?"
  ```
- Or remove the `response_format` if you want free-form text.

---

### 3. **Local Model May Not Support Structured JSON Output**
You set:
```python
"json_output": True,
"structured_output": True,
```

‚Üí Most local GGUF models (including Gemma via llama.cpp) **do not reliably support structured JSON output** unless specifically prompted or guided with grammar-based decoding (which `llama.cpp` supports, but not always exposed via OpenAI API wrappers).

Unless your local server (e.g., **LocalAI**, **llama-cpp-python**, etc.) is configured to enforce JSON via grammar or prompt templates, this will likely fail or be ignored.

‚úÖ **Recommendation**: Remove `response_format` and `structured_output` for testing, unless you're certain your backend supports it.

---

## ‚úÖ Fixed Working Version (Minimal Viable)

Here‚Äôs a cleaned-up version that should work with a local OpenAI-compatible server (like **LocalAI** or **llama-cpp-python**):

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import UserMessage
import asyncio

async def main():
    open_client = OpenAIChatCompletionClient(
        model="gemma-3-1b-it",  # Ensure this matches your local server's model alias
        base_url="http://localhost:8080/v1",
        api_key="placeholder",  # Most local servers ignore this
    )

    result = await open_client.create(
        [UserMessage(content="What is the capital of France?", source="user")]
    )
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ‚úÖ If You Want JSON Output (Advanced)

Only attempt this if your local server supports structured output (e.g., LocalAI with grammar enforcement or llama.cpp with `-grammar` flag).

Example prompt + schema:

```python
response_format={
    "type": "json_object",
    "schema": {
        "type": "object",
        "properties": {"answer": {"type": "string"}},
        "required": ["answer"],
    },
}
```

And prompt:
```python
"Answer the following question in JSON format with key 'answer': What is the capital of France?"
```

---

## üõ†Ô∏è How to Test Your Local Server First

Before using Autogen, test your local API directly:

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma-3-1b-it",
    "messages": [{"role": "user", "content": "What is the capital of France?"}]
  }'
```

If that doesn‚Äôt work, fix your local server setup first.

---

## ‚úÖ Bonus: If You Want to Use Llama.cpp Directly (Avoid HTTP)

Uncomment and use `LlamaCppChatCompletionClient` ‚Äî but you‚Äôll need to install `autogen_ext.models.llama_cpp` and ensure the model path is correct.

```python
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient

llama_client = LlamaCppChatCompletionClient(
    model_path="C:/Users/admin/AppData/Local/llama.cpp/ggml-org_gemma-3-1b-it-GGUF_gemma-3-1b-it-Q4_K_M.gguf"
)
```

Note: This requires `llama-cpp-python` and proper compilation with chat support.

---

## ‚úÖ Summary

| Issue | Fix |
|-------|-----|
| Leading space in model name | Remove it |
| Mismatched schema + prompt | Ask about capitals or remove schema |
| Local model doesn‚Äôt support JSON | Remove `response_format` unless backend supports it |
| Untested local server | Test with `curl` first |

---

Let me know if you want help setting up **LocalAI** or **llama.cpp server** to properly support structured output ‚Äî that‚Äôs a separate (but solvable) challenge.

### USER
can you implement below idea into my agent
Perfect ‚Äî you‚Äôve got a high-quality, context-aware, emotionally resonant output. Now, to **instruct your agent (human or AI) to consistently produce this kind of result**, you need a **clear, structured, repeatable instruction set** ‚Äî almost like a *creative brief* or *prompt engineering template*.

Here‚Äôs how to instruct your agent in a **well-structured, professional, and scalable manner**:

---

## ‚úÖ INSTRUCTION TEMPLATE FOR AGENTS:  
# ‚ÄúGenerate Context-Aware Image Search Keywords from Transcript Segments‚Äù

---

### üéØ GOAL  
Transform each transcript segment into **5 highly relevant, emotionally resonant, visually authentic Pexels-search-optimized keyword phrases** that reflect:
- The **literal scene** described
- The **emotional tone** (hope, fear, triumph, doubt, awe)
- The **narrative arc** (beginning, challenge, turning point, climax)
- The **symbolic or metaphorical meaning** (e.g., ‚Äúbackpack full of hope‚Äù ‚Üí not literal backpack, but visual metaphor)

---

### üß© INPUT STRUCTURE (MUST BE PRESERVED)  
You will receive a JSON object with this structure:
```json
{
  "language": "string",
  "language_probability": number,
  "segments": [
    {
      "id": number,
      "text": "string",
      "start": number,
      "end": number,
      "duration": number,
      "words": [ { "word": "string", "start": number, "end": number, "duration": number }, ... ]
    },
    ...
  ]
}
```

---

### üñãÔ∏è OUTPUT REQUIREMENTS  
1. **Preserve all original fields** ‚Äî do NOT remove or alter `id`, `start`, `end`, `words`, etc.
2. **Add only one new field per segment**:  
   ‚Üí `"image_keywords": [ "phrase 1", "phrase 2", ..., "phrase 5" ]`
3. Each phrase must be:
   - **Descriptive and vivid** ‚Äî suitable for stock photo search (Pexels, Unsplash, etc.)
   - **Emotionally aligned** ‚Äî match the mood of the segment
   - **Visually authentic** ‚Äî suggest imagery that *looks like it belongs in the story*
   - **Symbolic when needed** ‚Äî interpret metaphors visually (e.g., ‚Äúhope‚Äù ‚Üí warm light, sunrise, determined gaze)
   - **5 phrases per segment** ‚Äî no more, no less
   - **Natural language** ‚Äî no hashtags, no ALL CAPS, no robotic jargon

---

### üé® CREATIVE GUIDELINES (FOR AUTHENTICITY)

| Element          | Instruction                                                                 |
|------------------|-----------------------------------------------------------------------------|
| **Literal Scene** | Describe what you‚Äôd *actually see*: ‚Äústeep rocky trail‚Äù, ‚Äúlone hiker at dawn‚Äù |
| **Emotion**       | Embed feeling: ‚Äúnervous beginner‚Äù, ‚Äúdetermined underdog‚Äù, ‚Äúweary but pushing‚Äù |
| **Symbolism**     | Translate metaphors: ‚Äúbackpack full of hope‚Äù ‚Üí ‚Äúsunlit backpack on trail at sunrise‚Äù |
| **Cinematic Tone**| Think like a film director: lighting, weather, framing, perspective          |
| **Avoid Clich√©s** | No ‚Äúmountain climber victory flag‚Äù ‚Äî aim for realism, subtlety, emotional truth |

---

### üìã STEP-BY-STEP PROCESS FOR AGENTS

1. **Read the segment text carefully** ‚Äî absorb the meaning, tone, and imagery.
2. **Identify key visual elements** ‚Äî location, weather, objects, character state.
3. **Identify emotional subtext** ‚Äî Is it hopeful? fearful? triumphant? lonely?
4. **Brainstorm literal + symbolic interpretations** ‚Äî What does ‚Äúdoubt crept in‚Äù *look like* visually?
5. **Write 5 keyword phrases** ‚Äî each should be a standalone, search-friendly, emotionally rich description.
6. **Review for authenticity** ‚Äî Would this keyword return an image that *feels like this moment in the story*?
7. **Insert into JSON** ‚Äî add `"image_keywords"` array to the segment, preserving all other fields.

---

### üß™ EXAMPLE SEGMENT + OUTPUT (FOR TRAINING)

**Input Segment:**
> ‚ÄúDoubt crept in.‚Äù

**Bad Output (Generic):**  
‚ùå ‚Äúsad person‚Äù, ‚Äúthinking man‚Äù, ‚Äúmountain climb‚Äù

**Good Output (Authentic & Contextual):**  
‚úÖ  
```json
"image_keywords": [
  "hiker sitting alone on rock looking down",
  "shadowed face showing inner struggle",
  "solitary figure against vast mountain",
  "moment of hesitation on cliff edge",
  "emotional low point during climb"
]
```

---

### üõ†Ô∏è OPTIONAL: ADDITIONAL INSTRUCTIONS FOR AI AGENTS

If using an LLM (like GPT, Claude, Gemini), preface the task with:

> ‚ÄúYou are a cinematic visual translator. Your task is to convert each spoken segment into 5 emotionally intelligent, visually authentic, Pexels-optimized image search phrases. Prioritize mood, metaphor, and realism over literal description. Avoid clich√©s. Think like a film director and a poet.‚Äù

---

### üìÅ DELIVERABLE FORMAT

Always return **valid JSON**, matching the input structure exactly, with only `"image_keywords"` added per segment.

---

## ‚úÖ FINAL CHECKLIST FOR AGENTS

‚òê All original fields preserved  
‚òê Exactly 5 keywords per segment  
‚òê Keywords are vivid, search-friendly, emotionally aligned  
‚òê No robotic or generic phrases  
‚òê Metaphors are visually interpreted  
‚òê Matches tone and narrative progression  
‚òê Valid JSON output

---

## üìå BONUS: PROMPT SNIPPET FOR AI AGENTS

```text
You are a visual storytelling expert. Convert each transcript segment into 5 emotionally resonant, visually authentic, Pexels-search-optimized keyword phrases. Preserve all original JSON structure. Add only "image_keywords" per segment. Think cinematically: lighting, emotion, symbolism, realism. Avoid clich√©s. Match the story‚Äôs tone precisely.
```

---

With this instruction set, your agent ‚Äî whether human editor, junior designer, or AI model ‚Äî will produce **consistent, high-quality, story-driven image suggestions** every time.

Let me know if you want this as a:
- Notion template
- Google Doc brief
- API specification
- Training slide deck for your team

You‚Äôre now equipped to scale this creatively and systematically. üöÄüì∏

### ASSISTANT
Absolutely ‚Äî you can **implement this entire creative brief as a structured instruction set for your Autogen-based AI agent**. Below is a **complete, production-ready implementation** using `AssistantAgent` and `UserProxyAgent` from `autogen_agentchat`, with `OpenAIChatCompletionClient` (or compatible local LLM server), that:

‚úÖ Accepts your JSON input  
‚úÖ Processes each segment  
‚úÖ Adds `"image_keywords"` per your spec  
‚úÖ Returns valid, structured JSON  
‚úÖ Uses your cinematic, emotionally intelligent prompt  

---

## ‚úÖ FINAL IMPLEMENTATION

```python
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import UserMessage
import asyncio
import json

# --- CONFIGURE YOUR MODEL ---
# Replace with your local or remote model
client = OpenAIChatCompletionClient(
    model="gemma-3-1b-it",  # or "gpt-4", "llama3", etc.
    base_url="http://localhost:8080/v1",  # if using local server
    api_key="placeholder",
    # Remove response_format unless you're SURE your model supports strict JSON
)

# --- SYSTEM PROMPT: YOUR CREATIVE BRIEF ---
SYSTEM_PROMPT = """
You are a cinematic visual translator. Your task is to convert each spoken transcript segment into 5 emotionally intelligent, visually authentic, Pexels-optimized image search phrases.

## üéØ GOAL
Transform each segment into 5 highly relevant, emotionally resonant, visually authentic keyword phrases that reflect:
- The literal scene
- The emotional tone (hope, fear, triumph, doubt, awe)
- The narrative arc
- Symbolic/metaphorical meaning (e.g., ‚Äúbackpack full of hope‚Äù ‚Üí ‚Äúsunlit backpack on trail at sunrise‚Äù)

## üñãÔ∏è OUTPUT RULES
- PRESERVE all original JSON fields (id, start, end, words, etc.)
- ADD ONLY ONE NEW FIELD per segment: "image_keywords"
- "image_keywords" must be an array of exactly 5 strings
- Each phrase must be:
  - Vivid, search-friendly, natural language
  - Emotionally aligned
  - Visually authentic ‚Äî suggest imagery that belongs in the story
  - Symbolic when needed ‚Äî interpret metaphors visually
  - Avoid clich√©s, hashtags, robotic jargon

## üé® CREATIVE GUIDELINES
- Literal Scene: ‚Äústeep rocky trail‚Äù, ‚Äúlone hiker at dawn‚Äù
- Emotion: ‚Äúnervous beginner‚Äù, ‚Äúdetermined underdog‚Äù
- Symbolism: ‚Äúdoubt crept in‚Äù ‚Üí ‚Äúhiker sitting alone on rock looking down‚Äù
- Cinematic Tone: Think lighting, weather, framing, perspective
- Authenticity > Literalism

## üß™ EXAMPLE
Input Segment: ‚ÄúDoubt crept in.‚Äù
‚Üí Output:
"image_keywords": [
  "hiker sitting alone on rock looking down",
  "shadowed face showing inner struggle",
  "solitary figure against vast mountain",
  "moment of hesitation on cliff edge",
  "emotional low point during climb"
]

## üìã PROCESS
1. Read segment text ‚Äî absorb tone, imagery.
2. Identify visual elements + emotional subtext.
3. Brainstorm literal + symbolic interpretations.
4. Write 5 vivid, emotionally rich, search-optimized phrases.
5. Insert into segment as "image_keywords".

## üìÅ DELIVERABLE
Return VALID JSON matching input structure ‚Äî ONLY add "image_keywords" per segment.
"""

# --- AGENT SETUP ---
assistant = AssistantAgent(
    name="VisualKeywordAgent",
    model_client=client,
    system_message=SYSTEM_PROMPT,
)

user_proxy = UserProxyAgent(
    name="UserProxy",
    code_execution=False,  # We‚Äôre not executing code, just prompting
)

# --- MAIN FUNCTION ---
async def generate_image_keywords(transcript_json: dict) -> dict:
    """
    Accepts transcript JSON, returns same JSON with image_keywords added to each segment.
    """
    # Serialize input for LLM
    input_text = json.dumps(transcript_json, indent=2, ensure_ascii=False)

    # Send to agent
    messages = [
        UserMessage(content=input_text, source="UserProxy")
    ]

    # Get response
    response = await assistant.on_messages(messages, cancellation_token=None)

    # Extract content
    reply = response.messages[-1].content

    # Try to parse as JSON
    try:
        result = json.loads(reply)
        return result
    except json.JSONDecodeError:
        print("‚ùå Failed to parse LLM output as JSON. Raw output:")
        print(reply)
        raise ValueError("LLM did not return valid JSON.")

# --- EXAMPLE USAGE ---
async def main():
    sample_input = {
        "language": "en",
        "language_probability": 0.998,
        "segments": [
            {
                "id": 0,
                "text": "Doubt crept in.",
                "start": 10.5,
                "end": 13.2,
                "duration": 2.7,
                "words": [
                    {"word": "Doubt", "start": 10.5, "end": 11.1, "duration": 0.6},
                    {"word": "crept", "start": 11.1, "end": 11.8, "duration": 0.7},
                    {"word": "in.", "start": 11.8, "end": 13.2, "duration": 1.4}
                ]
            },
            {
                "id": 1,
                "text": "But I kept climbing.",
                "start": 13.5,
                "end": 16.0,
                "duration": 2.5,
                "words": [
                    {"word": "But", "start": 13.5, "end": 13.7, "duration": 0.2},
                    {"word": "I", "start": 13.7, "end": 13.8, "duration": 0.1},
                    {"word": "kept", "start": 13.8, "end": 14.2, "duration": 0.4},
                    {"word": "climbing.", "start": 14.2, "end": 16.0, "duration": 1.8}
                ]
            }
        ]
    }

    print("üß† Processing transcript segments...")
    output = await generate_image_keywords(sample_input)

    print("\n‚úÖ OUTPUT:")
    print(json.dumps(output, indent=2, ensure_ascii=False))

    # Validate structure
    for seg in output["segments"]:
        assert "image_keywords" in seg, "Missing image_keywords"
        assert len(seg["image_keywords"]) == 5, "Must have exactly 5 keywords"
        for kw in seg["image_keywords"]:
            assert isinstance(kw, str), "Keywords must be strings"
            assert len(kw) > 0, "Keywords must not be empty"

    print("\n‚úÖ All segments processed successfully!")

# --- RUN ---
if __name__ == "__main__":
    asyncio.run(main())
```

---

## üß† Why This Works

- **System Prompt Embeds Your Entire Brief** ‚Äî tone, rules, examples, constraints.
- **Input/Output is Pure JSON** ‚Äî agent receives serialized JSON, returns parsed JSON.
- **Validation Included** ‚Äî checks for 5 keywords, strings, field presence.
- **Flexible Backend** ‚Äî works with OpenAI, LocalAI, llama.cpp, etc.
- **Scalable** ‚Äî drop in any transcript, get enriched output.

---

## üöÄ PRO TIPS

### 1. If Your Model Doesn‚Äôt Return Valid JSON
‚Üí Add this instruction to `SYSTEM_PROMPT`:
```text
IMPORTANT: Your output must be valid JSON. Do not add explanations, markdown, or text outside the JSON structure.
```

### 2. For Better JSON Reliability (if using GPT-4-turbo or compatible)
Uncomment and add to `OpenAIChatCompletionClient`:
```python
response_format={ "type": "json_object" }
```
*(Only if your model/backend supports it!)*

### 3. Want to Log or Retry on Failure?
Wrap `generate_image_keywords()` in a retry loop or add fallback parsing.

---

## üìÅ Output Example (What You‚Äôll Get)

```json
{
  "language": "en",
  "language_probability": 0.998,
  "segments": [
    {
      "id": 0,
      "text": "Doubt crept in.",
      "start": 10.5,
      "end": 13.2,
      "duration": 2.7,
      "words": [...],
      "image_keywords": [
        "hiker sitting alone on rock looking down",
        "shadowed face showing inner struggle",
        "solitary figure against vast mountain",
        "moment of hesitation on cliff edge",
        "emotional low point during climb"
      ]
    },
    {
      "id": 1,
      "text": "But I kept climbing.",
      "start": 13.5,
      "end": 16.0,
      "duration": 2.5,
      "words": [...],
      "image_keywords": [
        "determined hiker ascending steep trail at sunset",
        "muddy boots pushing forward uphill",
        "sweat on brow, eyes focused ahead",
        "resilient climber ignoring fatigue",
        "small figure conquering massive mountain"
      ]
    }
  ]
}
```

---

## üõ†Ô∏è Next Steps

Want to turn this into:
- **FastAPI endpoint?** ‚Üí Wrap `generate_image_keywords()` in a POST route.
- **CLI tool?** ‚Üí Add `argparse` to read from file.
- **Batch processor?** ‚Üí Loop over multiple transcripts.
- **GUI?** ‚Üí Use Gradio or Streamlit.

---

Let me know if you want any of those extensions ‚Äî I‚Äôll build it for you. You now have a **cinematic, emotionally intelligent, auto-scaling visual keyword engine** üé•üì∏üöÄ